\documentclass{beamer}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{datetime}
\usepackage{IEEEtrantools}
\usepackage{multirow}
\newdate{date}{27}{10}{2018}
\newdateformat{mydateformat}{\monthname[\THEMONTH]\, \ordinaldate{\THEDAY}\,\THEYEAR}

\bibliographystyle{apalike}


\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



\newcommand{\Ba}{\mathbf{a}}    \newcommand{\Bb}{\mathbf{b}}    \newcommand{\Bc}{\mathbf{c}}    \newcommand{\Bd}{\mathbf{d}}    \newcommand{\Be}{\mathbf{e}}    \newcommand{\Bf}{\mathbf{f}}    \newcommand{\Bg}{\mathbf{g}}    \newcommand{\Bh}{\mathbf{h}}    \newcommand{\Bi}{\mathbf{i}}    \newcommand{\Bj}{\mathbf{j}}    \newcommand{\Bk}{\mathbf{k}}    \newcommand{\Bl}{\mathbf{l}}
\newcommand{\Bm}{\mathbf{m}}    \newcommand{\Bn}{\mathbf{n}}    \newcommand{\Bo}{\mathbf{o}}    \newcommand{\Bp}{\mathbf{p}}    \newcommand{\Bq}{\mathbf{q}}    \newcommand{\Br}{\mathbf{r}}    \newcommand{\Bs}{\mathbf{s}}    \newcommand{\Bt}{\mathbf{t}}    \newcommand{\Bu}{\mathbf{u}}    \newcommand{\Bv}{\mathbf{v}}    \newcommand{\Bw}{\mathbf{w}}    \newcommand{\Bx}{\mathbf{x}}
\newcommand{\By}{\mathbf{y}}    \newcommand{\Bz}{\mathbf{z}}    
\newcommand{\BA}{\mathbf{A}}    \newcommand{\BB}{\mathbf{B}}    \newcommand{\BC}{\mathbf{C}}    \newcommand{\BD}{\mathbf{D}}    \newcommand{\BE}{\mathbf{E}}    \newcommand{\BF}{\mathbf{F}}    \newcommand{\BG}{\mathbf{G}}    \newcommand{\BH}{\mathbf{H}}    \newcommand{\BI}{\mathbf{I}}    \newcommand{\BJ}{\mathbf{J}}    \newcommand{\BK}{\mathbf{K}}    \newcommand{\BL}{\mathbf{L}}
\newcommand{\BM}{\mathbf{M}}    \newcommand{\BN}{\mathbf{N}}    \newcommand{\BO}{\mathbf{O}}    \newcommand{\BP}{\mathbf{P}}    \newcommand{\BQ}{\mathbf{Q}}    \newcommand{\BR}{\mathbf{R}}    \newcommand{\BS}{\mathbf{S}}    \newcommand{\BT}{\mathbf{T}}    \newcommand{\BU}{\mathbf{U}}    \newcommand{\BV}{\mathbf{V}}    \newcommand{\BW}{\mathbf{W}}    \newcommand{\BX}{\mathbf{X}}
\newcommand{\BY}{\mathbf{Y}}    \newcommand{\BZ}{\mathbf{Z}}   






\theoremstyle{plain}
\newtheorem{proposition}{ Proposition}
\newtheorem{assumption}{ Assumption}

\theoremstyle{definition}
\newtheorem{remark}{Remark}
\theoremstyle{remark}

\usetheme{CambridgeUS}
\title{Integrated Likelihood Ratio Test}
\date{\mydateformat{\displaydate{date}}}
%\author{Rui Wang\inst{1} \and Xingzhong Xu\inst{1}}
\author{Rui Wang \and Xingzhong Xu}
\institute[BIT]{%\inst{1}
    %wangruiphd@bit.edu.cn
    %\inst{2}
    %xuxz@bit.edu.cn\\
    School of Mathematics and Statistics\\ Beijing Institute of Technology\\
}


\begin{document}
\maketitle
\section{The Likelihood Ratio Test}

\begin{frame}{Notations}
    \begin{itemize}
        \item
            Data $\BX^{(n)}=(X_1,\ldots,X_n)$ are iid from distribution $P_{\theta}$, $\theta\in\Theta$.
        \item 
    $P_{\theta}^{n}$: the joint distribution of $\BX^{(n)}$.
        \item
    $p_{n}(\BX^{(n)}|\theta)=\prod_{i=1}^n p(X_i|\theta)$: the density of $P_{\theta}^n$.
\item
$\theta=(\nu^T,\xi^T)^T$, where $\nu$ is $p_0$ dimensional and $\xi$ is $p-p_0$ dimensional.
\item
The null space $\Theta_0$ is a $p_0$-dimensional subspace of $\Theta$ defined as
\begin{equation*}
    \Theta_0=\{(\nu^T,\xi^T)^T:(\nu^T,\xi^T)^T\in\Theta, \, \xi=\xi_0\}.
\end{equation*}
\item
 Consider testing the hypotheses
\begin{equation*}
    H:\theta\in\Theta_0\quad \text{v.s.}\quad K:\theta\in \Theta_1=\Theta\backslash \Theta_0,
\end{equation*}
\item
If the null hypothesis is true, we denote by $\theta_0=(\nu_0^T,\xi_0^T)^T$ the true parameter which generates the data.
    \end{itemize}
\end{frame}
\begin{frame}{The likelihood ratio test}

    \begin{itemize}
        \item 
    For simple hypotheses $\Theta_0=\{\theta_0\}$ and $\Theta_0=\{\theta_1\}$, the most powerful test rejects the null hypothesis when $L(\theta_1)/L(\theta_0)$ is large
    (Neyman and Pearson).
\item
    The likelihood ratio test (LRT) is an extension of the idea of Neyman and Pearson for composite $\Theta_0$ and $\Theta_1$. 
\item
    The LRT statistic is defined as
    \begin{equation*}
        \Lambda_{\text{LRT}}=\frac{\max_{\theta \in \Theta}p_n(\BX^{(n)}|\theta)}{\max_{\theta \in \Theta_0}p_n(\BX^{(n)}|\theta)}.
    \end{equation*}
    \end{itemize}




    %The LRT is by no means the most powerful test. In fact, there is no most powerful test in general.
\end{frame}


\begin{frame}{Wilks' phenomenon of the LRT}
    Suppose data  $\BX^{(n)}=(X_1,\ldots,X_n)$ are iid from distribution $P_{\theta}$, $\theta\in\Theta$. The parameter space $\Theta$ is an open subset of $\mathbb{R}^p$ and $\Theta_0$ is a $p_0$-dimensional subset of $\mathbb{R}^p$.

    \cite{Wilks1938The} proved that under certain regular conditions,
    \begin{equation*}
        2\log \Lambda_{\text{LRT}}\overset{P_{\theta_0}^n}{\rightsquigarrow} \chi^2(p-p_0),
    \end{equation*}
    where $\theta_0$ is a relative inner point of $\Theta_0$.

    Wilks' phenomenon can be used to determine the critical value of the LRT statistic.
\end{frame}


\begin{frame}{Weakness of the LRT}
    Although the LRT has been widely used, it does not have good behavior in some models.
    \begin{itemize}
        \item
            If the likelihood function is not concave, the implementation of the LRT may not be easy.
            For example, EM algorithm may trap in a local maxima.
        \item
            In some cases, the theoretical properties of the LRT are not satisfactory.
        \item
            For certain problems, the likelihood function is unbounded.
            Consequently, the LRT is not defined.
    \end{itemize}

\end{frame}

\begin{frame}{An example of unbounded likelihood}
        Suppose $X_1,\ldots,X_n$ are iid from the mixture model 
        \begin{equation*}
        (1-\omega)\mathcal{N}(0,1)+\omega\mathcal{N}(\xi,\sigma^2).
        \end{equation*}
        The likelihood function is
        \begin{equation*}
            p_n(\BX^{(n)}|\omega,\xi,\sigma^2)=\prod_{i=1}^n \left((1-\omega)\phi(X_i)+ \frac{\omega}{\sigma} \phi\left(\frac{X_i-\xi}{\sigma}\right)\right).
        \end{equation*}
        Let $\omega=1/2$, $\xi=X_1$, then
        \begin{equation*}
            p_n(\BX^{(n)}|1/2,X_1,\sigma^2)
            \geq
            \frac{1}{2^n}
            \frac{1}{\sigma} \phi(0)\prod_{i=2}^n \phi(X_i),
        \end{equation*}
        which tends to infinity as $\sigma^2\to 0$.

\end{frame}


\begin{frame}{An example of unbounded likelihood}
    
\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{figure/newnewPP}
    \end{center}
    \caption{
        The likelihood function in $-\log (\sigma^2)$ with $\omega=1/2$ and $\xi=X_1$.
    Data are generate from the normal mixture model with $(\omega,\xi,\sigma^2)^T=(1/2,1,10^{-2})$ and $n=50$.
    }
    \label{myFigure1}
\end{figure}
\end{frame}


\begin{frame}{An example of unbounded likelihood}

    \begin{itemize}
        \item
    From the figure, we can see that the likelihood tends to infinity as $-\log (\sigma^2)$ tends to infinity, i.e., $\sigma^2$ tends to $0$.
        \item 
        The failure of the LRT does not mean the likelihood function can not be used, it just implies the maximum of the likelihood function can not be used.
        \item
    In fact, the likelihood has good \textbf{local} behavior.
        We can see that the likelihood has a local maximum around the true parameter $-\log (\sigma^{2})=-\log (10^{-2})$.
    \item
        See~\cite{Cam1990Maximum} for more examples.
    \end{itemize}


\end{frame}
\section{Integrated Likelihood Ratio Test}
\begin{frame}{Motivation}
    \begin{itemize}
        \item 
In goodness of fit test, there are two common types of tests: extreme value type (Kolmogorov-Smirnov test, e.g.) and integral type (Cram\'er-von Mises test, e.g.).
\item
In classical parametric hypothesis testing, no attention has been paid to the integrated likelihood functions.
    \end{itemize}

\end{frame}    


\begin{frame}{Motivation}
A natural integral type test statistic is
\begin{equation}\label{naiLiS} 
    \frac{\int_{\Theta}p_n(\BX^{(n)}|\theta) d\Pi(\theta)}{\int_{\Theta_0} p_n(\BX^{(n)}|\theta) d\Pi^{(0)}(\theta)},
\end{equation}
where $\Pi$ and $\Pi^{(0)}$ are some probability measures on $\Theta$ and $\Theta_{0}$, respectively.

\end{frame}
\begin{frame}{Motivation}
    \begin{itemize}
        \item
If $\Pi$ and $\Pi^{(0)}$ are independent of data, then the statistic~\eqref{naiLiS} is exactly the Bayes factor \cite{scientificInference} with the prior distributions $\Pi$ and $\Pi^{(0)}$.
\item
The asymptotic distribution of Bayes factor depends on the prior density at the true parameter; see, e.g.,~\cite{clarke1990information}.
Consequently, the Bayes factor \textbf{cannot} be treated as a frequentist test statistic.
\item
    The measures $\Pi$ and $\Pi^{(0)}$ considered in this paper will \textbf{depend on data}.
\end{itemize}
\end{frame}

\begin{frame}{Motivation}
    \begin{itemize}
        \item 
If $\Pi(\theta=\hat{\theta}_{\text{MLE}})=1$ and $\Pi^{(0)}(\theta=\hat{\theta}^{(0)}_{\text{MLE}})=1$, then the statistic~\eqref{naiLiS} becomes the LRT statistic.
\item
 For many models where the LRT fails, the smoother $\Pi$ and $\Pi^{(0)}$ with small tail probability may be better choices.
 \item
Following this idea, a natural choice is to take $\Pi$ and $\Pi^{(0)}$ as the posterior distribution (with certain prior distributions) of $\theta$ in $\Theta$ and $\Theta_0$, respectively.
\item
In this case, the statistic~\eqref{naiLiS} becomes the posterior Bayes factor proposed (PBF) by~\cite{Aitkin1991Posterior}.
    \end{itemize}

\end{frame}

\begin{frame}{Generalized fractional Bayes factor}
    
    \begin{itemize}
        \item 
If we replace the likelihood function $L(\theta)$ by $L(\theta)^a$ for $a>0$, then the LRT statistic becomes
\begin{equation*}
    \frac{\max_{\theta\in\Theta}L^{a}(\theta)}{\max_{\theta\in\Theta_0}L^a(\theta)}=\Lambda^a_{\text{LRT}},
\end{equation*}
%Hence the LRT statistic is equivariant if we raise the likelihood to the power of $a$.
which is equivalent to the LRT statistic.
\item
In contrast, the statistic
\begin{equation}\label{naiLiS2}
    \frac{\int_{\Theta} \big[p_n(\BX^{(n)}|\theta)\big]^{a}\pi(\theta;\BX^{(n)})\,d\theta}{\int_{\tilde{\Theta}_0} \big[p_n(\BX^{(n)}|\nu,\xi_0)\big]^{a}\pi(\nu;\BX^{(n)})\,d\nu},
\end{equation}
is not equivalent to the statistic~\eqref{naiLiS}.
We call~\eqref{naiLiS2} the integrated likelihood ratio test (ILRT) statistic.
    \end{itemize}
\end{frame}

\begin{frame}{Generalized fractional Bayes factor}
\begin{itemize}
\item
We also consider the test statistic~\eqref{naiLiS2} with $0<a<1$.
    \item 
Correspondingly, the measure $\Pi$ and $\Pi^{(0)}$ can also take the fractional posterior~\cite{Bha2016}.
\begin{equation}\label{firstWeight}
    \begin{split}
\pi(\theta;\BX^{(n)})&=\frac{\big[p_n(\BX^{(n)}|\theta)\big]^b \pi(\theta)}{\int_{\Theta}\big[p_n(\BX^{(n)}|\theta)\big]^b \pi(\theta)\, d\theta},
\\
\pi(\nu;\BX^{(n)})&=\frac{\big[p_n(\BX^{(n)}|\nu,
    \xi_0)\big]^b \pi(\nu)}{\int_{\tilde{\Theta}_0}\big[p_n(\BX^{(n)}|\nu,\xi_0)\big]^b \pi(\nu)\, d\nu},
\end{split}
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}{Generalized fractional Bayes factor}

    \begin{itemize}
        \item
    With fractional likelihood and fractional posterior as weight function, the ILRT statistic equalts to
$$
    \Lambda_{a,b}=
    \frac{L_{a+b}}{L_{b}}\cdot \frac{L_{b}^{(0)}}{L_{a+b}^{(0)}},
$$
where for $t>0$,
 $$
 L_t=\int_{\Theta}\big[p_n(\BX^{(n)}|\theta)\big]^t \pi(\theta)\, d\theta,\quad
 L_t^{(0)}=\int_{\Theta_0}\big[p_n(\BX^{(n)}|\nu,\xi_0)\big]^t \pi(\nu)\, d\nu.
 $$
\item
    Note that for $a\in(0,1)$, $\Lambda_{a,1-a}$ is exactly the fractional Bayes factor (FBF) proposed by \cite{Fractional1995}.
Hence we call $\Lambda_{a,b}$ the generalized FBF.
    \end{itemize}
\end{frame}
\begin{frame}{Asymptotic behavior of the generalized FBF}
    
    \textbf{Regular conditions}

    \begin{enumerate}[(i)]
    \item
    The parameter spacec $\Theta$ and $\tilde{\Theta}_0$ are open subsets of $\mathbb{R}^p$ and $\mathbb{R}^{p_0}$, respectively.
    The parameters $\theta_0$ and $\nu_0$ are inner points of $\Theta$ and $\tilde{\Theta}_0$, respectively.
\item
    The derivative 
$$\dot{\ell}_{\theta_0}(X)=\frac{\partial}{\partial \theta}\log p(X|\theta)\Big|_{\theta=\theta_0}$$
exists $P_{\theta_0}$-a.s.\ and satisfies $P_{\theta_0}\dot{\ell}_{\theta_0}=0_p$.
\item
The Fisher information matrix $I_{\theta_0}=P_{\theta_0}\dot{\ell}_{\theta_0}\dot{\ell}_{\theta_0}^T$ is positive-definite.
\item
For every $M>0$,
    \begin{equation*}
        \sup_{\|h\|\leq M}\Big|
         \log \frac{p_n(\BX^{(n)}|\theta_0+n^{-1/2}h)}{p_n(\BX^{(n)}|\theta_0)}-h^T I_{\theta_0}\Delta_{n,\theta_0}+\frac{1}{2}h^T I_{\theta_0}h
         \Big|\xrightarrow{P^n_{\theta_0}}0,
    \end{equation*}
    where $\Delta_{n,\theta_0}=n^{-1/2}\sum_{i=1}^n I_{\theta_0}^{-1}\dot{\ell}_{\theta_0}(X_i)$.
\end{enumerate}

\end{frame}
\begin{frame}{Asymptotic behavior of the generalized FBF}
    \begin{itemize}
        \item
    For a set $A\subset \Theta$, denote $L_t (A)=\int_{A} [ {p_n(\BX^{(n)}|\theta)} ]^{t} \pi(\theta) \, d \theta$.
        \item
    For $t>0$, we say $L_t$ is $\sqrt{n}$-consistent if for every $M_n\to \infty$,
    $$
    \frac{L_t({\{\theta:\|\theta-\theta_0\|> M_n/\sqrt{n}\}})}{L_t}\xrightarrow{P_{\theta_0}^n} 0,
    $$
\item
    The $\sqrt{n}$-consistency of $L_1$ is exactly the $\sqrt{n}$-consistency of the posterior distribution.
            \item
    The $\sqrt{n}$-consistency of $L_t^{(0)}$ is similarly defined.
\end{itemize}
\end{frame}


\begin{frame}{Asymptotic behavior of the generalized FBF}

    \textbf{Theorem}

        Suppose that
        \begin{enumerate}[(i)]
            \item
                Regular conditions hold.
            \item
                $L_{a+b}$, $L_b$, $L_{a+b}^{(0)}$ and $L_b^{(0)}$ are $\sqrt{n}$-consistent.
            \item
                $\pi(\theta)$ is continuous at $\theta_0$ with $\pi(\theta_0)>0$, $\pi(\nu)$ is continuous at $\nu_0$ with $\pi(\nu_0)>0$.
            \end{enumerate}
                Then for $\{\theta_n\}$ such that $\sqrt{n}(\theta_n-\theta_0)\to \eta$, 
        $$
        2\log \Lambda_{a,b}\overset{P^n_{\theta_n}}{\rightsquigarrow}-{(p-p_0)}\log (1+\frac{a}{b})+{a}\chi^2(p-p_0,\delta),
        $$
        where $\chi^2(p-p_0,\delta)$ is a noncentral chi-squared random variable with $p-p_0$ degrees of freedom and noncentrality parameter $\delta=\eta^T\big( I_{\theta_0}-I_{\theta_0} J(J^T I_{\theta_0} J)^{-1}J^T I_{\theta_0}\big)\eta$ and $J=(I_{p_0},0_{p_0\times(p-p_0)})^T$,
``$\rightsquigarrow$'' means weak convergence.
\end{frame}
\begin{frame}{Asymptotic behavior of the generalized FBF}
    \begin{itemize}
        \item
    This theorem is the Wilks' phenomenon for the generalized FBF.
        \item
    To formulate a test with asymptotic type I error rate $\alpha$, the critical value of $2\log \Lambda_{a,b}$ can be defined to be $-(p-p_0)\log (1+a/b)+ a\chi^2_{1-\alpha}(p-p_0)$.
        \item
The resulting test has local asymptotic power
\begin{equation*}
\Pr \left( \chi^2(p-p_0,\delta)> \chi^2_{1-\alpha}(p-p_0) \right).
\end{equation*}
    \end{itemize}
\end{frame}
\begin{frame}{$\sqrt{n}$-consistency of $L_t$}
    \begin{itemize}
        \item
    The key condition for our theorem is the $\sqrt{n}$-consistency of $L_t$.
        \item
    For full-rank exponential family, $L_t$ is $\sqrt{n}$-consistent.
    \end{itemize}
\begin{proposition}\label{exponentialCon}
    Suppose $p(X|\theta)=\exp\big[\theta^T T(X)-A(\theta)\big]$, $\Theta$ is an open subset of $\mathbb{R}^p$, $\theta_0$ is an interior point of $\Theta$, 
    $$I_{\theta_0}=\frac{\partial^2}{\partial \theta \partial \theta^T} A(\theta_0)>0.$$
    Then $L_{t}$ is consistent for $t>0$.
\end{proposition}
\end{frame}
\begin{frame}{$\sqrt{n}$-consistency of $L_t$}
    However, $L_t$ is not always $\sqrt{n}$-consistent in general. In fact, for $t>1$, $L_t$ is not always well defined.
\begin{proposition}
    If $t\leq 1$, $L_t< +\infty$ $P_{\theta_0}^n$-a.s. for any models. If $t> 1$, $L_t = +\infty$ for some models.
    \label{exprop}
\end{proposition}
Hence, we will only consider $t\leq 1$.
\end{frame}
\begin{frame}{$\sqrt{n}$-consistency of $L_t$}
    \begin{itemize}
        \item
    For $t=1$, the $\sqrt{n}$-consistency of $L_t$ is equivalent to the $\sqrt{n}$-consistency of the posterior distribution which is a well studied problem; see, e.g.,~\cite{ghosal2000,Shen2001Rates,vaart2007convergence}.
        \item
A popular and convenient way of establishing the consistency of posterior is through the condition that suitable test sequences exist.
        \item
For example, Theorem 3.1 of \cite{Kleijn2012The} assumes that for every $\epsilon>0$, there exists a sequence of tests $\phi_n$ such that
\begin{equation}\label{eq:testSeq}
    P_{\theta_0}^n\phi_n\to 0,\quad \sup_{\|\theta-\theta_0\|\geq \epsilon} P_\theta^n(1-\phi_n)\to 0.
\end{equation}
        \item
However, if the parameter space is not compact, one may have to manually construct a test sequence satisfying the condition~\eqref{eq:testSeq}.
\end{itemize}
\end{frame}


\begin{frame}{$\sqrt{n}$-consistency of $L_t$}
    \begin{itemize}
        \item
The consistency of $L_t$ for $0<t<1$ is different from $t=1$.
        \item
\cite{kar10563} considered the Hellinger consistency of $L_{1/2}$.
\item
%They only consider $t=1/2$ and didn't consider the $\sqrt{n}$-convergence result.
Recently,~\cite{Bha2016} further developed the idea of~\cite{kar10563} and derived a general bounds for the consistency of $L_t$ for $0<t<1$.
\item
However, their result can not yield the $\sqrt{n}$-consistency for parametric models.
\end{itemize}
\end{frame}
\begin{frame}{$\sqrt{n}$-consistency of $L_t$}
 For two parameters $\theta_1$ and $\theta_2$, the $\alpha$ order R\'{e}nyi divergence ($0<\alpha<1$) of $P_{\theta_1}$ from $P_{\theta_2}$ is defined to be
$$
D_{\alpha}(\theta_1||\theta_2)=-\frac{1}{1-\alpha}\log \rho_{\alpha}(\theta_1,\theta_2),
$$
where
$
\rho_{\alpha}(\theta_1,\theta_2)=\int_{\mathcal{X}} p(X|\theta_1)^{\alpha} p(X|\theta_2)^{1-\alpha} \, d \mu
$ is the so-called Hellinger integral.
\end{frame}
\begin{frame}{$\sqrt{n}$-consistency of $L_t$}

\begin{assumption}\label{Assumption4}
    For some $\alpha\in(0,1)$, there exist positive constants $\delta$, $\epsilon$ and $C$ such that,
     $D_{\alpha}(\theta||\theta_0)  \geq  C \|\theta-\theta_0\|^2$ for $\|\theta-\theta_0\|\leq \delta$ and $D_{\alpha}(\theta||\theta_0) \geq \epsilon$ for $\|\theta-\theta_0\|>\delta$.
\end{assumption}
This assumption is a fairly weak condition.
\begin{proposition}\label{Theoremless1}
    Suppose $\theta_0$ is an interior of $\Theta$, $\pi(\theta)$ is continuous at $\theta_0$ and $\pi(\theta_0)>0$.
    Under regular conditions and the above assumption, for fixed $t\in(0,1)$, $L_t$ is consistent.
\end{proposition}

\end{frame}

\begin{frame}{$\sqrt{n}$-consistency of $L_t$}
    \begin{itemize}
        \item
If the conditions of the theorem are satisfied, the asymptotic power of $\Lambda_{a,b}$ is independent of $a,b$.
        \item
Hence specific choices of $a,b$ are not crucial provided $L_{a+b}$, $L_b$, $L^{(0)}_{a+b}$ and $L^{(0)}_b$ are $\sqrt{n}$-consistent.
        \item
For some models, it is more convenient to verify Assumption~\ref{Assumption4} than to directly construct a test sequence satisfying the condition~\eqref{eq:testSeq}.
In such cases, it can be recommended to use the generalized FBF with $a+b< 1$.
\end{itemize}
\end{frame}

\section{General Weight Function}

\begin{frame}{General weight function}
    \begin{itemize}
        \item
For some moderately complex models, the fractional posterior~\eqref{firstWeight} are not easy to calculate.
        \item
Idea: use simpler weight functions to approximate~\eqref{firstWeight}.
\end{itemize}
Let $h=\sqrt{n}(\theta-\theta_0)$ and $\pi_n(h;\BX^{(n)})=\pi(\theta_0+n^{-1/2}h;\BX^{(n)})$ be the weight function in terms of $h$.

\textbf{Bernstein-von Mises theorem:}
The posterior density $\pi(h|\BX^{(n)})$ satisfies
$$
            \|\pi_n(h|\BX^{(n)})-\phi(h;\Delta_{n,\theta_0},I_{\theta_0}^{-1})\|
            \xrightarrow{P_{\theta_0}^n}0.
$$

%Then the posterior density of $h$ is $\pi_n(h|\BX^{(n)})=n^{-1/2}\pi(\theta|\BX^{(n)})$.
%Theorem 2.1 of \cite{Kleijn2012The},

\end{frame}

\begin{frame}{General weight function}
Similarly, if $\pi(\theta;\BX^{(n)})$ is the fractional posterior density of $\theta$ with fractional power $b$, it can be proved that under certain conditions,
$$
\|\pi_n(h;\BX^{(n)})-\phi(h;\Delta_{n,\theta_0},b^{-1} I_{\theta_0}^{-1})\|
            \xrightarrow{P_{\theta_0}^n}0.
$$

\end{frame}
\begin{frame}{General weight function}
\begin{assumption}\label{Assumption3}
    Let $b\in(0,1)$ be a fixed number.
    Assume that $\pi_n(h;\BX^{(n)})$ satisfies
        \begin{equation}\label{vonMisesResults}
            \|\pi_n(h;\BX^{(n)})-\phi(h;\Delta_{n,\theta_0},b^{-1}I_{\theta_0}^{-1})\|\overset{P_{\theta_0}^n}{\to}0.
        \end{equation}

        Similarly,  assume that 
        $
    \|\pi_n(h^{(0)};\BX^{(n)})-\phi(h^{(0)};\Delta^{(0)}_{n,\theta_0},b^{-1}I_{\theta_0}^{(0)-1})\|\overset{P_{\theta_0}^n}{\to}0.
    $
Furthermore, assume that for every $\epsilon>0$, there exists Lebesgue integrable functions $T(h)$ and $T^{(0)}(h)$ such that 

    \begin{equation}\label{Assump31}
        \lim_{n\to \infty}P_{\theta_0}^n\left\{\sup_{h\in \mathbb{R}^p}(\pi_n(h;\BX^{(n)})-T(h))\leq 0\right\}\geq 1-\epsilon.
\end{equation}
    \begin{equation}\label{Assump31l}
        \lim_{n\to \infty}P_{\theta_0}^n\left\{\sup_{h^{(0)}\in \mathbb{R}^{p_0}}(\pi_n(h^{(0)};\BX^{(n)})-T^{(0)}(h^{(0)}))\leq 0\right\}\geq 1-\epsilon.
\end{equation}
\end{assumption}
\end{frame}
\begin{frame}{General weight function}
Under this assumption, we consider the ILRT statistic
\begin{equation*}
    \Lambda^*_{a,b}=\frac{\int_{\Theta} \big[p_n(\BX^{(n)}|\theta)\big]^{a}\pi(\theta;\BX^{(n)})\,d\theta}{\int_{\tilde{\Theta}_0} \big[p_n(\BX^{(n)}|\nu,\xi_0)\big]^{a}\pi(\nu;\BX^{(n)})\,d\nu}.
\label{eq:definition}
\end{equation*}
\end{frame}
\begin{frame}{General weight function}
\begin{theorem}\label{theoremMain}
    Suppose that the regular conditions and the above assumption hold with $a+b\leq 1$.
    Then for $\{\theta_n\}$ such that $\sqrt{n}(\theta_n-\theta_0)\to \eta$, we have

        $$
        2\log \Lambda^*_{a,b}\overset{P^n_{\theta_n}}{\rightsquigarrow}-{(p-p_0)}\log (1+\frac{a}{b})+{a}\chi^2(p-p_0,\delta).
        $$
\end{theorem}
\end{frame}
\begin{frame}{General weight function}
    \begin{itemize}
        \item
Theorem~\ref{theoremMain} shows that even with approximate weight function, the ILRT statistic can still produce an asymptotic optimal test.
\item
A practical method to obtain simple form weight function $\pi_n(h;\BX^{(n)})$ is the variational inference; see, e.g.,~\cite{blei2017}.
\item
Next we shall consider a simple variational method which is guaranteed to yield a weight function satisfying Assumption~\ref{Assumption3}.
\item
For comprehensive considerations of the statistical properties of variational methods; see the recent works of~\cite{yixin2017},~\cite{pati2017} and~\cite{yunyang2017}.
\end{itemize}
\end{frame}
\begin{frame}{General weight function}
    \begin{itemize}
        \item 
$\mathcal{Q}$: the family of all $p$ dimensional normal distribution.
\item
$\pi(\theta;\BX^{(n)})$: the fractional posterior of order $b$.
\item
$\pi_n(h;\BX^{(n)})=n^{-1/2}\pi(\theta_0+n^{-1/2}h;\BX^{(n)})$ satisfies
\begin{equation*}\label{eq:xiebuwanlaaa}
    \|\pi_n(h;\BX^{(n)})-\phi(h;\Delta_{n,\theta_0},b^{-1}I_{\theta_0}^{-1})\| \xrightarrow{P_{\theta_0}^n}0.
\end{equation*}
    \end{itemize}
\end{frame}
\begin{frame}{General weight function}
    \begin{itemize}
        \item 
$\pi^{\dagger}(\theta;\BX^{(n)})$: the normal approximation of $\pi(\theta;\BX^{(n)})$ obtained from R\'{e}nyi divergence variational inference~\cite{NIPS2016_6208}, that is,
    $$
    \pi^{\dagger}(\theta;\BX^{(n)})=\argmin_{q(\theta)\in\mathcal{Q}} -\frac{1}{1-\alpha} \log \int q(\theta)^{\alpha} \pi(\theta;\BX^{(n)})^{1-\alpha}\, d\theta,
    $$
    where $0<\alpha<1$ is an arbitrary constant.
        \item
    The weight function $\pi^{\dagger}(\theta;\BX^{(n)})$ can produce a ILRT satisfying the same asymptotic properties.
    \end{itemize}
\end{frame}
\section{Normal mixture model}
\begin{frame}{Normal mixture model}
    \begin{itemize}
        \item 
Normal mixture model is a highly irregular model.
        \item
Loss of identifiability.
\item
If the component variances are totally unknown, the likelihood is unbounded and thus likelihood ratio test is not defined \cite{Cam1990Maximum}.
\item
See~\cite{chenjiahua2017} for a review of the testing problems for mixture models.
\item
Since the integral of the likelihood can smooth the irregular behavior of the likelihood, it can be expected that ILRT may have better behavior than likelihood ratio test.
\item
For unknown variances case, ILRT is well defined.
    \end{itemize}
\end{frame}
\begin{frame}{Normal mixture model}
    \begin{itemize}
        \item 
$X_1,\ldots,X_n$: iid with density
\begin{equation*}
    p(X|\omega,\xi,\sigma^2)=\frac{1-\omega}{\sqrt{2\pi}}\exp\big(-\frac{1}{2}X^2\big)
+\frac{\omega}{\sqrt{2\pi}\sigma}\exp\big(-\frac{1}{2\sigma^2}(X-\xi)^2\big),
\end{equation*}
where $0\leq \omega \leq 1$, $\mu\in \mathbb{R}$ and $\sigma^2\in \mathbb{R}^+$.
\item
First, we assume $\omega=1/2$ is known and consider testing the hypotheses
\begin{equation*}
    H: \xi=0,\sigma=1\quad \text{vs.} \quad K: \xi\neq 0 \text{ or } \sigma \neq 1.
    \label{mixturehy1}
\end{equation*}
\item
    For this testing problem, the likelihood function is {\color{red}unbounded} under the alternative hypothesis.
    (take $\xi=X_1$ and let $\sigma^2\to 0$)
    \end{itemize}
\end{frame}
\begin{frame}{Normal mixture model}
Using our general theorems, we can obtain the following proposition.
\begin{proposition}
For the above hypotheses testing problem, 
If $\sqrt{n}((\xi,\sigma^2)-(0,1))^T\to (\eta_1,\eta_2)^T $, 
then the generalized FBF with $a+b<1$ satisfies
\begin{equation*}
    2\log \Lambda_{a,b}\overset{P^n_{\theta_n}}{\rightsquigarrow}-2\log (1+\frac{a}{b})+{a}\chi^2(2,\eta_1^2/4+\eta_2^2/8).
\end{equation*}
    \label{propositionTT}
\end{proposition}
\end{frame}
\begin{frame}{Normal mixture model}
    \begin{itemize}
        \item 
If $\omega$ in unknown, then the mixture model suffers from loss of identifiability and the behavior of the likelihood is fairly complicated.
        \item
Now we assume $\sigma^2=1$ is known and consider testing the hypotheses
\begin{equation*}
    \omega \xi=0
    \quad \text{vs.}\quad
    \omega \xi \neq 0.
    \label{newHy}
\end{equation*}
\item
Although the LRT exists in this problem,~\cite{HALL2005158} showed that it has trivial power under $n^{-1/2}$ local alternative hypothesis. 
    \end{itemize}
\end{frame}
\begin{frame}{Normal mixture model}
    \begin{itemize}
        \item 
For this irregular problem, our general theorems cannot be directly applied.
\item
This is because the second part is Assumption~\eqref{Assumption3} is violated due to loss of identifiability.
\item
However, this does not means that the ILRT is not applicable.
    \end{itemize}
\end{frame}
\begin{frame}{Normal mixture model}
\begin{theorem}
    Suppose $\pi(\omega,\xi)=\pi_{\omega}(\omega) \pi_{\xi}(\xi)$, $\pi_\xi(\xi)$ is positive and continuous at $\xi=0$,
    $\pi_\omega(\omega)\sim \text{Beta}(\alpha_1,\alpha_2)$ with $\alpha_1>1$.
    Suppose $a+b<1$.
    Then,
    \begin{enumerate}[(i)]
        \item
    under the null hypothesis,
    \begin{equation*}
        2\log \Lambda_{a,b} \overset{P^n_{\theta_0}}{\rightsquigarrow}\log(1+\frac a b)+ a\chi^2(1);
    \end{equation*}
\item
    suppose for some $s<1/4$, $\omega \geq n^{-s}$ for large $n$, $\sqrt{n}\omega \xi \to \eta$, then
    \begin{equation*}
        2\log \Lambda_{a,b} \overset{P^n_{\theta_n}}{\rightsquigarrow}\log(1+\frac a b)+ a\chi^2(1,\eta^2).
    \end{equation*}
\end{enumerate}
    \label{mixtureThm}
\end{theorem}
\end{frame}


\begin{frame}{Discussion}

\begin{itemize}
    \item
        The ILRT can have good behavior even if the LRT is not defined or has poor properties.
    \item 
The ILRT statistic is easy to implement provided sampling from weight functions is simple.
\item
If the weight functions are fractional posterior densities, then Markov chain Monte Carlo (MCMC) methods can be used to sample from weight functions.
\item
If MCMC is not efficient, one can use approximation methods such as variational inference and the resulting test procedure is still valid.
\item
Thus, the ILRT methodology can also be recommended when the classical LRT is not easy to implement.
%The integral can smooth the likelihood.
%Hence it can be expected that the ILRT method can have better properties than the LRT when the likelihood has complicated behavior.
%The success of the ILRT methodology in our examples verifies this point.
\item
It is interesting to apply the ILRT methodology to specific complex testing problems.
We leave it for future research.
\end{itemize}
\end{frame}
\begin{frame}

        \centerline{
            \huge
    Thank You!
}
\vspace{1cm}
        \centerline{
            \huge
            Questions?
}
\end{frame}

\bibliography{mybibfile}
\end{document}
